{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec-finetuning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE_xL0u4xKR_"
      },
      "source": [
        "training_dataset_path = '' # leave empty if using pretrained model, place model in models/ dir\n",
        "finetuning_dataset_path = 'lyrics_digital_fingerprint.parquet' # dataset suppposed to be in parquet, text should be the last column of dataset\n",
        "pretrained_model_name = '' # leave empty if you don't have pretrained model\n",
        "models_dir = 'models/'\n",
        "n_lines = 1 # 0 to not split lines, 1,2,3 to split lyrics into 1,2,3-lines chunks\n",
        "num_epochs_ft = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teSmXS_AH8XF"
      },
      "source": [
        "# !pip install gensim==3.8.1\n",
        "# !pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ph6Ts5fcJB3d"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from datetime import datetime\n",
        "import gensim\n",
        "from gensim.models import Word2Vec \n",
        "from gensim.models import KeyedVectors\n",
        "from time import time \n",
        "from nltk.tokenize import RegexpTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QneLZgE0KoSR"
      },
      "source": [
        "def split_to_n_lines(text, n=1):\n",
        "    if n > 0:\n",
        "        lines_iter = iter(text.splitlines())\n",
        "        lines = []\n",
        "        for date_time_order in zip(lines_iter):\n",
        "            lines.append(\" \".join(date_time_order))\n",
        "        return lines\n",
        "    else: \n",
        "        return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2uK5wqSJe7i"
      },
      "source": [
        "def load_dataset(dataset_path, n_lines):\n",
        "    stopwords = {'estribillo', 'x2', 'x3', 'x4', 'x5', 'coro-estribillo', '(x2)', '(x3)', '(x4)', '()',\n",
        "                 '(repite estribillo 3 veces)'}\n",
        "    REGEX_PAT = re.compile(r\"\\b(?:\" + (\"|\").join(stopwords) + \")\\\\b\", re.IGNORECASE)\n",
        "    df = pd.read_parquet(dataset_path, columns=['lyrics'])\n",
        "    df[\"lyrics\"] = df[\"lyrics\"].str.replace(REGEX_PAT, \"\", regex=True)\n",
        "    df['lyrics'] = df.apply(lambda row: split_to_n_lines(row['lyrics'], n_lines), axis=1)\n",
        "    df = df.explode('lyrics')\n",
        "    df['lyrics'] = df.apply(lambda row: (row['lyrics'].strip()), axis=1)\n",
        "    df = df[df['lyrics'].apply(len) > 10]\n",
        "    df = df[df['lyrics'].apply(len) < 1000]\n",
        "    sentences = df['lyrics'].tolist()\n",
        "    print(\"Train sentences:\", len(sentences))\n",
        "    return sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWM-CD52ZsAD"
      },
      "source": [
        "def collect_pretrained_model(training_dataset_path, pretrained_model_name, models_dir):\n",
        "    if pretrained_model_name:\n",
        "        if os.path.isdir(models_dir + pretrained_model_name):\n",
        "            model_path = [f for f in listdir(models_dir + pretrained_model_name) if f.endswith('.model')]\n",
        "            model = gensim.models.Word2Vec.load(model_path[0])\n",
        "            print('model: ' + model_path[0] + ' loaded')\n",
        "            return model\n",
        "        \n",
        "    elif training_dataset_path:\n",
        "        training_dataset = load_dataset(training_dataset_path, n_lines)\n",
        "        training_dataset_tokenized = tokenize_dataset(training_dataset)\n",
        "        model = gensim.models.Word2Vec(\n",
        "        training_dataset,\n",
        "        size=400,\n",
        "        window=5,\n",
        "        min_count=2,)\n",
        "        print('model initialized with data from: ' + training_dataset_path )\n",
        "        return model\n",
        "    \n",
        "    else: \n",
        "\n",
        "        if not os.path.isdir(models_dir + 'complete_model/'):\n",
        "            os.makedirs(models_dir + 'complete_model/')\n",
        "            print(\"Tworze folder\")\n",
        "            url = 'https://zenodo.org/record/1410403/files/complete_model.zip?download=1'\n",
        "            zip_path, _ = urllib.request.urlretrieve(url)\n",
        "            with zipfile.ZipFile(zip_path, \"r\") as f:\n",
        "                f.extractall(models_dir + 'complete_model/')\n",
        "        print('aitoralmeida model loading...')\n",
        "        return gensim.models.Word2Vec.load(models_dir + 'complete_model/complete.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qxuq8RhUK5cj"
      },
      "source": [
        "def tokenize_dataset(sentences):\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    sentences_tokenized = [w.lower() for w in sentences]\n",
        "    sentences_tokenized = [tokenizer.tokenize(i) for i in sentences_tokenized]\n",
        "    return sentences_tokenized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IXDSKFgMdgE"
      },
      "source": [
        "def finetune_word2vec(model, sentences_tokenized):\n",
        "    vocab = len(model.wv.vocab)\n",
        "    print(vocab)\n",
        "    model.build_vocab(sentences_tokenized, update=True)\n",
        "    len(model.wv.vocab)\n",
        "    print(vocab)\n",
        "    model.train(sentences_tokenized, total_examples=model.corpus_count, epochs=num_epochs_ft)\n",
        "    print('Your model is finetuned now!!!')\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe6D8CzOUqZF"
      },
      "source": [
        "def save_model(model, model_path):\n",
        "    output_dir = \"output/{}-{}/\".format(model_path.replace(\"/\", \"_\").replace('.parquet','') , datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    model.save(output_dir + \"gensim.model\")\n",
        "    model.wv.save_word2vec_format(output_dir + \"word2vec.vector\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl1hD9nmYdYA"
      },
      "source": [
        "model = collect_pretrained_model(training_dataset_path, pretrained_model_name, models_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ptup0Ytdsv2"
      },
      "source": [
        "if finetuning_dataset_path:\n",
        "    sentences = load_dataset(finetuning_dataset_path, n_lines)\n",
        "    sentences_tokenized = tokenize_dataset(sentences)\n",
        "    model_ft = finetune_word2vec(model, sentences_tokenized)\n",
        "    save_model(model_ft, finetuning_dataset_path)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}